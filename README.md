**Neural Network Forward Propagation**

Welcome to my GitHub repository for exploring forward propagation in neural networks! ðŸ§ 

In this repository, you'll find an IPython Notebook (`forward_propagation.ipynb`) where I delve into various concepts crucial for forward propagation, including activation functions. Here's a glimpse of what you'll discover:

### Concepts Covered:
- **Activation Function**: I demonstrate the implementation of the sigmoid activation function using NumPy.
- **Network Initialization**: Learn how to initialize the neural network for forward propagation.
- **Weighted Sum Computation**: Explore the calculation of the weighted sum for each node in the network.
- **Node Activation**: Witness the computation of node activation using the activation function.
- **Forward Propagation**: Understand the process of propagating input data forward through the network.

### Usage:
1. Clone the repository to your local machine.
2. Open `forward_propagation.ipynb` using Jupyter Notebook or Google Colab.
3. Run the cells to understand and experiment with forward propagation concepts.

### Dependencies:
Make sure you have NumPy installed. If not, simply run:
```bash
!pip install numpy
```

Feel free to explore, experiment, and expand upon the concepts presented here. Happy coding! ðŸš€

If you have any questions or suggestions, don't hesitate to reach out. ðŸ“©
